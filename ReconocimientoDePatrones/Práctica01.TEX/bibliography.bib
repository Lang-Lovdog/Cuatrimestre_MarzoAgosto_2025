@BOOK{M2006-tk,
  title     = {Pattern Recognition and Machine Learning},
  author    = {M., Christopher},
  publisher = {Springer},
  series    = {Information Science and Statistics},
  edition   = {1},
  month     = {aug},
  year      = {2006},
  address   = {New York, NY},
  language  = {en}
}

@BOOK{Jain1988-ab,
  title     = {Fundamentals of digital image processing},
  author    = {Jain, Anil K},
  publisher = {Pearson},
  month     = {sep},
  year      = {1988},
  address   = {Upper Saddle River, NJ},
  language  = {en}
}

@article{Bishop2006,
  title={Pattern recognition and machine learning},
  author={Bishop, Christopher M.},
  year={2006},
  publisher={Springer}
}

@article{Zhou2012,
  title={Ensemble methods: foundations and algorithms},
  author={Zhou, Zhi-Hua},
  year={2012},
  publisher={CRC Press}
}

@article{Pedregosa2011,
  title={Scikit-learn: Machine learning in Python},
  author={Pedregosa, Fabian and Varoquaux, Ga{\"e}l and Gramfort, Alexandre and Michel, Vincent and Thirion, Bertrand and Grisel, Olivier and Blondel, Mathieu and Prettenhofer, Peter and Weiss, Ron and Dubourg, Vincent and others},
  journal={Journal of Machine Learning Research},
  volume={12},
  pages={2825--2830},
  year={2011}
}

@article{Breiman2001,
  title={Random forests},
  author={Breiman, Leo},
  journal={Machine Learning},
  volume={45},
  number={1},
  pages={5--32},
  year={2001},
  doi={10.1023/A:1010933404324}
}

@book{Duda2012,
  title={Pattern classification},
  author={Duda, Richard O. and Hart, Peter E. and Stork, David G.},
  year={2012},
  publisher={John Wiley \& Sons},
  edition={2nd}
}

@article{Quinlan1986,
  title={Induction of decision trees},
  author={Quinlan, J. Ross},
  journal={Machine Learning},
  volume={1},
  number={1},
  pages={81--106},
  year={1986},
  doi={10.1007/BF00116251}
}

@inbook{Mhlenbein1996,
  title = {From recombination of genes to the estimation of distributions I. Binary parameters},
  ISBN = {9783540706687},
  ISSN = {1611-3349},
  url = {http://dx.doi.org/10.1007/3-540-61723-X_982},
  DOI = {10.1007/3-540-61723-x_982},
  booktitle = {Parallel Problem Solving from Nature — PPSN IV},
  publisher = {Springer Berlin Heidelberg},
  author = {M\"{u}hlenbein,  H. and Paaß,  G.},
  year = {1996},
  pages = {178–187}
}

@book{EDAsPL2002,
  title = {Estimation of Distribution Algorithms},
  ISBN = {9781461515395},
  ISSN = {1568-2587},
  url = {http://dx.doi.org/10.1007/978-1-4615-1539-5},
  DOI = {10.1007/978-1-4615-1539-5},
  journal = {Genetic Algorithms and Evolutionary Computation},
  publisher = {Springer US},
  year = {2002}
}

@inproceedings{Caruana2008,
  series = {ICML ’08},
  title = {An empirical evaluation of supervised learning in high dimensions},
  url = {http://dx.doi.org/10.1145/1390156.1390169},
  DOI = {10.1145/1390156.1390169},
  booktitle = {Proceedings of the 25th international conference on Machine learning - ICML ’08},
  publisher = {ACM Press},
  author = {Caruana,  Rich and Karampatziakis,  Nikos and Yessenalina,  Ainur},
  year = {2008},
  pages = {96–103},
  collection = {ICML ’08}
}

@article{KOHAVI1997273,
  title = {Wrappers for feature subset selection},
  journal = {Artificial Intelligence},
  volume = {97},
  number = {1},
  pages = {273-324},
  year = {1997},
  note = {Relevance},
  issn = {0004-3702},
  doi = {https://doi.org/10.1016/S0004-3702(97)00043-X},
  url = {https://www.sciencedirect.com/science/article/pii/S000437029700043X},
  author = {Ron Kohavi and George H. John},
  keywords = {Classification, Feature selection, Wrapper, Filter},
  abstract = {In the feature subset selection problem, a learning algorithm is faced with the problem of selecting a relevant subset of features upon which to focus its attention, while ignoring the rest. To achieve the best possible performance with a particular learning algorithm on a particular training set, a feature subset selection method should consider how the algorithm and the training set interact. We explore the relation between optimal feature subset selection and relevance. Our wrapper method searches for an optimal feature subset tailored to a particular algorithm and a domain. We study the strengths and weaknesses of the wrapper approach and show a series of improved designs. We compare the wrapper approach to induction without feature subset selection and to Relief, a filter approach to feature subset selection. Significant improvement in accuracy is achieved for some datasets for the two families of induction algorithms used: decision trees and Naive-Bayes.}
}

@article{QUEMY2020101483,
  title = {Two-stage optimization for machine learning workflow},
  journal = {Information Systems},
  volume = {92},
  pages = {101483},
  year = {2020},
  issn = {0306-4379},
  doi = {https://doi.org/10.1016/j.is.2019.101483},
  url = {https://www.sciencedirect.com/science/article/pii/S0306437919305356},
  author = {Alexandre Quemy},
  keywords = {Data pipelines, Hyperparameter tuning, AutoML, CASH},
  abstract = {Machine learning techniques play a preponderant role in dealing with massive amount of data and are employed in almost every possible domain. Building a high quality machine learning model to be deployed in production is a challenging task, from both, the subject matter experts and the machine learning practitioners. For a broader adoption and scalability of machine learning systems, the construction and configuration of machine learning workflow need to gain in automation. In the last few years, several techniques have been developed in this direction, known as AutoML. In this paper, we present a two-stage optimization process to build data pipelines and configure machine learning algorithms. First, we study the impact of data pipelines compared to algorithm configuration in order to show the importance of data preprocessing over hyperparameter tuning. The second part presents policies to efficiently allocate search time between data pipeline construction and algorithm configuration. Those policies are agnostic from the metaoptimizer. Last, we present a metric to determine if a data pipeline is specific or independent from the algorithm, enabling fine-grain pipeline pruning and meta-learning for the coldstart problem.}
}

@misc{https://doi.org/10.24432/c5mc89,
  doi = {10.24432/C5MC89},
  url = {https://archive.ics.uci.edu/dataset/697},
  author = {Valentim Realinho,  Mónica Vieira Martins},
  title = {Predict Students' Dropout and Academic Success},
  publisher = {UCI Machine Learning Repository},
  year = {2021}
}

@article{article/2024_Anderson_Thomas_Adelusi_Joshua,
  author = {Anderson, Thomas and Adelusi, Joshua},
  year = {2024},
  month = {10},
  pages = {},
  title = {Data Cleaning and Preprocessing for Noisy Datasets: Techniques, Challenges, and Solutions}
}

@book{9780691652214_Richard-E.-Bellman,
  author = {Richard E. Bellman},
  title = {Adaptive Control Processes: A Guided Tour},
  year = {1961},
  publisher = {Princeton University Press}
}

@inproceedings{deap_2012,
  author = {De Rainville, François-Michel and Fortin, Félix-Antoine and Gardner, Marc-André and Parizeau, Marc and Gagné, Christian},
  year = {2012},
  month = {07},
  pages = {85-92},
  title = {DEAP: A Python framework for Evolutionary Algorithms},
  journal = {GECCO'12 - Proceedings of the 14th International Conference on Genetic and Evolutionary Computation Companion},
  doi = {10.1145/2330784.2330799}
}

@article{sklearn_10.5555/1953048.2078195,
  author = {Pedregosa, Fabian and Varoquaux, Ga\"{e}l and Gramfort, Alexandre and Michel, Vincent and Thirion, Bertrand and Grisel, Olivier and Blondel, Mathieu and Prettenhofer, Peter and Weiss, Ron and Dubourg, Vincent and Vanderplas, Jake and Passos, Alexandre and Cournapeau, David and Brucher, Matthieu and Perrot, Matthieu and Duchesnay, \'{E}douard},
  title = {Scikit-learn: Machine Learning in Python},
  year = {2011},
  issue_date = {2/1/2011},
  publisher = {JMLR.org},
  volume = {12},
  number = {null},
  issn = {1532-4435},
  abstract = {Scikit-learn is a Python module integrating a wide range of state-of-the-art machine learning algorithms for medium-scale supervised and unsupervised problems. This package focuses on bringing machine learning to non-specialists using a general-purpose high-level language. Emphasis is put on ease of use, performance, documentation, and API consistency. It has minimal dependencies and is distributed under the simplified BSD license, encouraging its use in both academic and commercial settings. Source code, binaries, and documentation can be downloaded from http://scikit-learn.sourceforge.net.},
  journal = {J. Mach. Learn. Res.},
  month = nov,
  pages = {2825–2830},
  numpages = {6}
}

@ARTICLE{Evolving_NN6790655,
  author={Stanley, Kenneth O. and Miikkulainen, Risto},
  journal={Evolutionary Computation}, 
  title={Evolving Neural Networks through Augmenting Topologies}, 
  year={2002},
  volume={10},
  number={2},
  pages={99-127},
  keywords={Genetic algorithms;neural networks;neuroevolution;network topologies;speciation;competing conventions},
  doi={10.1162/106365602320169811}
}

@inproceedings{EfficientRobust_10.5555/2969442.2969547,
  author = {Feurer, Matthias and Klein, Aaron and Eggensperger, Katharina and Springenberg, Jost Tobias and Blum, Manuel and Hutter, Frank},
  title = {Efficient and robust automated machine learning},
  year = {2015},
  publisher = {MIT Press},
  address = {Cambridge, MA, USA},
  abstract = {The success of machine learning in a broad range of applications has led to an ever-growing demand for machine learning systems that can be used off the shelf by non-experts. To be effective in practice, such systems need to automatically choose a good algorithm and feature preprocessing steps for a new dataset at hand, and also set their respective hyperparameters. Recent work has started to tackle this automated machine learning (AutoML) problem with the help of efficient Bayesian optimization methods. Building on this, we introduce a robust new AutoML system based on scikit-learn (using 15 classifiers, 14 feature preprocessing methods, and 4 data preprocessing methods, giving rise to a structured hypothesis space with 110 hyperparameters). This system, which we dub AUTO-SKLEARN, improves on existing AutoML methods by automatically taking into account past performance on similar datasets, and by constructing ensembles from the models evaluated during the optimization. Our system won the first phase of the ongoing ChaLearn AutoML challenge, and our comprehensive analysis on over 100 diverse datasets shows that it substantially outperforms the previous state of the art in AutoML. We also demonstrate the performance gains due to each of our contributions and derive insights into the effectiveness of the individual components of AUTO-SKLEARN.},
  booktitle = {Proceedings of the 29th International Conference on Neural Information Processing Systems - Volume 2},
  pages = {2755–2763},
  numpages = {9},
  location = {Montreal, Canada},
  series = {NIPS'15}
}

@ARTICLE{TakingHumanOut7352306,
  author={Shahriari, Bobak and Swersky, Kevin and Wang, Ziyu and Adams, Ryan P. and de Freitas, Nando},
  journal={Proceedings of the IEEE}, 
  title={Taking the Human Out of the Loop: A Review of Bayesian Optimization}, 
  year={2016},
  volume={104},
  number={1},
  pages={148-175},
  keywords={Big data;Bayes methods;Linear programming;Decision making;Design of experiments;Optimization;Genomes;Statistical analysis;decision making;design of experiments;optimization;response surface methodology;statistical learning;genomic medicine;Decision making;design of experiments;optimization;response surface methodology;statistical learning},
  doi={10.1109/JPROC.2015.2494218}
}

@ARTICLE{NoFreeLunch6795940,
  author={Wolpert, David H.},
  journal={Neural Computation}, 
  title={The Lack of A Priori Distinctions Between Learning Algorithms}, 
  year={1996},
  volume={8},
  number={7},
  pages={1341-1390},
  keywords={},
  doi={10.1162/neco.1996.8.7.1341}
}

@book{AutoMLBook2019,
  title = {Automated Machine Learning: Methods,  Systems,  Challenges},
  ISBN = {9783030053185},
  ISSN = {2520-1328},
  url = {http://dx.doi.org/10.1007/978-3-030-05318-5},
  DOI = {10.1007/978-3-030-05318-5},
  journal = {The Springer Series on Challenges in Machine Learning},
  publisher = {Springer International Publishing},
  page = {3-5},
  year = {2019}
}

@book{AutoMLBook-CASH2019,
  title = {Automated Machine Learning: Methods,  Systems,  Challenges},
  ISBN = {9783030053185},
  ISSN = {2520-1328},
  url = {http://dx.doi.org/10.1007/978-3-030-05318-5},
  DOI = {10.1007/978-3-030-05318-5},
  journal = {The Springer Series on Challenges in Machine Learning},
  publisher = {Springer International Publishing},
  chapter = {4.3},
  year = {2019}
}

@inbook{RobFeatSelSaeys2008,
  title = {Robust Feature Selection Using Ensemble Feature Selection Techniques},
  ISBN = {9783540874812},
  ISSN = {1611-3349},
  url = {http://dx.doi.org/10.1007/978-3-540-87481-2_21},
  DOI = {10.1007/978-3-540-87481-2_21},
  booktitle = {Machine Learning and Knowledge Discovery in Databases},
  publisher = {Springer Berlin Heidelberg},
  author = {Saeys,  Yvan and Abeel,  Thomas and Van de Peer,  Yves},
  year = {2008},
  pages = {313–325}
}

@inproceedings{10.5555/2955491.2955545,
  author = {Cant\'{u}-Paz, Erick},
  title = {Feature subset selection by estimation of distribution algorithms},
  year = {2002},
  isbn = {1558608788},
  publisher = {Morgan Kaufmann Publishers Inc.},
  address = {San Francisco, CA, USA},
  abstract = {This paper describes the application of four evolutionary algorithms to the selection of feature subsets for classification problems. Besides of a simple genetic algorithm (GA), the paper considers three estimation of distribution algorithms (EDAs): a compact GA, an extended compact GA, and the Bayesian Optimization Algorithm. The objective is to determine if the EDAs present advantages over the simple GA in terms of accuracy or speed in this problem. The experiments used a Naive Bayes classifier and public-domain and artificial data sets. All the algorithms found feature subsets that resulted in higher accuracies than using all the features. However, in contrast with other studies, we did not find evidence to support or reject the use of EDAs for this problem.},
  booktitle = {Proceedings of the 4th Annual Conference on Genetic and Evolutionary Computation},
  pages = {303–310},
  numpages = {8},
  location = {New York City, New York},
  series = {GECCO'02}
}

%%% References of wrapper subset selection applications
@article{Panthong2015,
  title = {Wrapper Feature Subset Selection for Dimension Reduction Based on Ensemble Learning Algorithm},
  volume = {72},
  ISSN = {1877-0509},
  url = {http://dx.doi.org/10.1016/j.procs.2015.12.117},
  DOI = {10.1016/j.procs.2015.12.117},
  journal = {Procedia Computer Science},
  publisher = {Elsevier BV},
  author = {Panthong,  Rattanawadee and Srivihok,  Anongnart},
  year = {2015},
  pages = {162–169}
}

@ARTICLE{WrappedBased-RapidImage5256251,
  author={Durbha, Surya S. and King, Roger L. and Younan, Nicolas H.},
  journal={IEEE Geoscience and Remote Sensing Letters}, 
  title={Wrapper-Based Feature Subset Selection for Rapid Image Information Mining}, 
  year={2010},
  volume={7},
  number={1},
  pages={43-47},
  keywords={Remote sensing;Genetic algorithms;Predictive models;Image retrieval;Image sensors;Sea measurements;Principal component analysis;Senior members;Information retrieval;Support vector machine classification;Coastal disasters;feature selection;genetic algorithms (GAs);rapid image information mining (RIIM);wrapper-based approaches},
  doi={10.1109/LGRS.2009.2028585}
}

@article{Choi2011,
  title = {Gene selection and prediction for cancer classification using support vector machines with a reject option},
  volume = {55},
  ISSN = {0167-9473},
  url = {http://dx.doi.org/10.1016/j.csda.2010.12.001},
  DOI = {10.1016/j.csda.2010.12.001},
  number = {5},
  journal = {Computational Statistics \& Data Analysis},
  publisher = {Elsevier BV},
  author = {Choi,  Hosik and Yeo,  Donghwa and Kwon,  Sunghoon and Kim,  Yongdai},
  year = {2011},
  month = may,
  pages = {1897–1908}
}

@article{10.5555/2188385.2188395,
  author = {Bergstra, James and Bengio, Yoshua},
  title = {Random search for hyper-parameter optimization},
  year = {2012},
  issue_date = {3/1/2012},
  publisher = {JMLR.org},
  volume = {13},
  number = {null},
  issn = {1532-4435},
  abstract = {Grid search and manual search are the most widely used strategies for hyper-parameter optimization. This paper shows empirically and theoretically that randomly chosen trials are more efficient for hyper-parameter optimization than trials on a grid. Empirical evidence comes from a comparison with a large previous study that used grid search and manual search to configure neural networks and deep belief networks. Compared with neural networks configured by a pure grid search, we find that random search over the same domain is able to find models that are as good or better within a small fraction of the computation time. Granting random search the same computational budget, random search finds better models by effectively searching a larger, less promising configuration space. Compared with deep belief networks configured by a thoughtful combination of manual search and grid search, purely random search over the same 32-dimensional configuration space found statistically equal performance on four of seven data sets, and superior performance on one of seven. A Gaussian process analysis of the function from hyper-parameters to validation set performance reveals that for most data sets only a few of the hyper-parameters really matter, but that different hyper-parameters are important on different data sets. This phenomenon makes grid search a poor choice for configuring algorithms for new data sets. Our analysis casts some light on why recent "High Throughput" methods achieve surprising success--they appear to search through a large number of hyper-parameters because most hyper-parameters do not matter much. We anticipate that growing interest in large hierarchical models will place an increasing burden on techniques for hyper-parameter optimization; this work shows that random search is a natural baseline against which to judge progress in the development of adaptive (sequential) hyper-parameter optimization algorithms.},
  journal = {J. Mach. Learn. Res.},
  month = feb,
  pages = {281–305},
  numpages = {25},
  keywords = {deep learning, global optimization, model selection, neural networks, response surface modeling}
}
@misc{ syT0826,
  author = { Raul E. S\'{a}nchez-Yañez},
  title  = {Tarea(para el 0826): Caractersticas a Partir de Histogramas de Sumas y de Diferencias},
  year   = {2025}
}

@BOOK{Jain1988-vs,
  title     = "Fundamentals of digital image processing",
  author    = "Jain, Anil K",
  publisher = "Pearson",
  month     =  sep,
  year      =  1988,
  address   = "Upper Saddle River, NJ",
  language  = "en"
}

@online{sonymage_TexturaVisual,
  title     = "Textura Visual: Ejemplos y Definicion",
  author    = "Sonymage",
  url       = "https://sonymage.es/textura-visual-ejemplos-y-definicion/?expand_article=1",
  year      = 2023
}

@online{drawpaintaVET,
  url       = "https://drawpaintacademy.com/visual-element-texture/",
  author    = "Dan Scott",
  title     = "Visual Element Texture",
  year      = 2018
}
