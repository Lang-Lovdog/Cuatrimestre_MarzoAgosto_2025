---
title: "Perceptron implementation"
execute:
   enabled: true
format:
   pdf:
     title: Brief history of neural networks and a perceptron implementation
     author: Brandon Marquez Salazar
     pdf-engine: pdflatex
     documentclass: IEEEtran
     number-sections: true
     bibliography: bibliography.bib
     bibliographystyle: ieee
     nocite: |
      @*
     cite-method: biblatex
     include-in-header:
       - text: |
           \usepackage{dirtytalk}
           \usepackage{tikz}
           \usetikzlibrary{positioning}
           \tikzset{
             x=1em,
             y=1em,
           }
           \input{Macros}
#  html: 
#    code-fold: true
#    theme: cyborg
jupyter: redesneuronales
---

```{=latex}
\input{MarquezSalazarBrandon-RedesNeuronales-Tarea01}
\section{A perceptrion implementation}
```

## Perceptron algorithm implementation

I'll use numpy as a very common package used for scientific computation,
**make_blobs** from Sci-kit learn for data generation and pyplot from
matplotlib for data visualization.

```{python}
#| output: false
!pip install matplotlib
```
```{python}
#| output: false
!pip install pandas
```
```{python}
#| output: false
!pip install scikit-learn
```
```{python}
#| output: false
import numpy as np
from sklearn.datasets import make_blobs
import matplotlib.pyplot as plt

```

Now, in order to implement a perceptron we have to define the three steps needed
and the evaluation function


## Neuron Model

First the perceptron output, which is the most important element, it's the neuron model.
It will receive a vector of inputs $X_{jk}$ a vector of weights $W_{jk}$ and a reference
for the activation function $S(\cdot)$.

```{python}
def perceptronOutput(W,X,bias,ActivationFunction):
  WeighedInputs = np.dot(W,X)
  Net = WeighedInputs - bias
  O = ActivationFunction(Net)
  return O
```


## Error equation

This function computes the error of the perceptron based on its output compared
to the desired output. It receives the perceptron output vector $O_j$,
the desired output vector $Y_j$ and the number of patterns $N$.

```{python}
def computeError(Y,O,N):
  DeltaSum = np.sum(np.abs(Y-O))
  Err = DeltaSum/N
  return Err
```


## Weights update function

This function receives the current weights vector $W_j$, the perceptron and desired
output vectors $O_j$ and $Y_j$ and learning rate $r$. Then returns the new weights vector.

```{python}
def updateWeights(Y,O,W,X,r):
  NewWeights = W - (Y-O)*X*r
  return  NewWeights
```


## Evaluation function

For this one, I'll create my own evaluation function based on a normalized [0,1]
sign function.

$S(X) = \begin{cases}
  1, &X >     0\\
  0, &X \leq  0\\
\end{cases}$

```{python}
def S(Net):
  if Net<1 :
    return 0;
  return 1;

```

## Creating input data

Using Scikit-Learn I'll create a new dataset of inputs and its desired
output (classification) in order to train our perceptron.

```{python}
# This one is to get 3000 elements to classify
N=3000
# Two features to input the perceptron
nFeatures=2
X,Y = make_blobs(
  # Number of elements  # Number of features
  n_samples=N,          n_features=nFeatures,
  # Two classes         # Standard deviation
  centers=2,            cluster_std=1.9,
  # Only to get randomly positioned
  shuffle=True,
  # Random Seed
  random_state=1201931
)
plt.scatter(X[:,0],X[:,1],c=Y,cmap='magma',s=5)
plt.ylabel('Feature 1')
plt.xlabel('Feature 2')
plt.title('Sample space')
plt.colorbar(label="Classes")
plt.show()
```

## Perceptron training and results

Then I'll derived of the features quantity, I'll create a weights vetor
$W$. And create a function which will process a datum.

```{python}
bias = -0.2
learnRate = 0.6
W=np.zeros(nFeatures)
for i in range(len(X)):
  Xi = X[i]; Yi = Y[i];
  Oi=perceptronOutput(Xi,W,bias,S)
  Err=computeError(Yi,Oi,N)
  W=updateWeights(Yi,Oi,W,Xi,learnRate)
```

Then I'll plot the sample space with a frontier made by the weights

```{python}
m=-W[0]/W[1]
Po=[ min(X[:,0]) , max(X[:,1]) ]
Pf=np.multiply(Po,m)+np.divide(bias,W[1])

plt.plot(Po,Pf,'g-',linewidth=0.8)
plt.scatter(X[:,0],X[:,1],c=Y,cmap='magma',s=5)
plt.ylabel('Feature 1')
plt.xlabel('Feature 2')
plt.title('Sample space')
plt.colorbar(label="Classes")
plt.show()
```

# Conclusion

Perceptron was the first approach to modern neural networks which came from
neuroscience intents to understand brain cells and how they work. This is an
actual advance in the field of Artificial Intelligence; however, a perceptron
is not quite sufficient enough to be used in complex applications, but linearly
separable problems can be solved with it. But being a good starting point for
more complex implementations.
